{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3893c255-5fe4-4dd7-9d17-2273051163ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Chroma DB と AWS Bedrock を使って、より高度な質問に対応する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f809159d-b739-4df0-b9fd-1648e38edf4b",
   "metadata": {},
   "source": [
    "このノートブックでは、Langchain, Chroma DB, AWS Bedrock の設定を行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d12bc0-e516-4ba7-bb5d-493e40de6fbb",
   "metadata": {},
   "source": [
    "- **AWS Bedrock の設定:** AWS の Bedrock のLLM が自然言語の理解と計算を担当します\n",
    "\n",
    "- **Chroma DB Client の初期化:** 次に、ベクターDBである Chroma DB との接続を確立します。\n",
    "\n",
    "- **Langchain の設定:** Langchainを設定します。Langchainは、言語モデルを外部のDBとシームレスに接続します。\n",
    "\n",
    "- **実践的なサンプルを実行:** 設定が完了したら、関数を実行します。この関数は、Langchain, Bedrock, Chroma DB をどのように組み合わせて質問に答えるかを実演しています。Chroma DB に格納されたベクトルデータを使い、言語モデルに関連する情報を与え、回答の正確性と関連度を強化しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de600824-8843-4d80-adf2-9373246ee4ab",
   "metadata": {},
   "source": [
    "### 6.1 AWS Bedrock の設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406bdb65-fcb0-4a8c-995f-1bee1f45cbca",
   "metadata": {},
   "source": [
    "まずは AWS Bedrock の設定から始めましょう。\n",
    "以下の関数では、あらかじめ指定された環境変数を Langchain の **default** として利用しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa759dd7-632d-4ebf-9215-df426ea094f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS の Credential を、以下のディレクトリに作成しました: /home/cdsw/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def create_aws_credentials_file():\n",
    "    # Retrieve environment variables\n",
    "    aws_region = os.environ.get('AWS_DEFAULT_REGION', 'default_region')\n",
    "    aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID', 'default_access_key')\n",
    "    aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY', 'default_secret_key')\n",
    "\n",
    "    # Define the folder and file paths\n",
    "    aws_folder_path = os.path.expanduser('~/.aws')\n",
    "    credentials_file_path = os.path.join(aws_folder_path, 'credentials')\n",
    "\n",
    "    # Create the .aws directory if it does not exist\n",
    "    if not os.path.exists(aws_folder_path):\n",
    "        os.makedirs(aws_folder_path)\n",
    "\n",
    "    # Write the credentials to the file\n",
    "    with open(credentials_file_path, 'w') as credentials_file:\n",
    "        credentials_file.write('[default]\\n')\n",
    "        credentials_file.write(f'aws_access_key_id={aws_access_key_id}\\n')\n",
    "        credentials_file.write(f'aws_secret_access_key={aws_secret_access_key}\\n')\n",
    "        credentials_file.write(f'region={aws_region}\\n')\n",
    "\n",
    "    print(f\"AWS の Credential を、以下のディレクトリに作成しました: {credentials_file_path}\")\n",
    "    \n",
    "create_aws_credentials_file()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c809e286-4346-4b11-bc41-5f31706dd09a",
   "metadata": {},
   "source": [
    "### 6.2 Chroma の初期設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79215a9-bbed-49c0-a223-8d962a3623ac",
   "metadata": {},
   "source": [
    "次に、Chroma DB を初期設定します。\n",
    "ベクターDBにデータを入れるためには、**5_populate_local_chroma_db/populate_chroma_vectors.py** を利用してください。\n",
    "\n",
    "コレクション名は **cml-default** とします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fd0b9c5-e997-41f8-b645-df7a0626b4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "persistent_client = chromadb.PersistentClient(path=\"/home/cdsw/chroma-data\")\n",
    "COLLECTION_NAME = \"cml-default\"\n",
    "\n",
    "collection = persistent_client.get_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a24f1ccd-7bcc-4f91-ac54-57a890369d4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "レコード数: 15\n"
     ]
    }
   ],
   "source": [
    "# レコード数の確認\n",
    "record_count = collection.count()\n",
    "print(f\"レコード数: {record_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520a6f5-6bc8-4b9e-9acc-c28f947e0903",
   "metadata": {},
   "source": [
    "Chroma DB にベクトルを格納するためのエンべディングモデルは、 [sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) を利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8c82968-9660-43b1-9b23-1d0e3a542926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "EMBEDDING_MODEL_REPO = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "EMBEDDING_MODEL_NAME = \"all-mpnet-base-v2\"\n",
    "\n",
    "EMBEDDING_FUNCTION = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82d9b240-07a6-4206-a77c-04d3fffc2374",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = EMBEDDING_FUNCTION.embed_query(\"Icebergって何ですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "696ec9c2-db1f-479a-a893-981dd16823f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query_embeddings の中身は 768次元のベクトル\n",
    "len(query_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1dbc38aa-ac75-47b2-b9fd-b83dbfbbfc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特定の条件を持つクエリ\n",
    "query_results = collection.query(\n",
    "    query_embeddings=query_embeddings,\n",
    "    n_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdfe3d99-9cc1-4b4e-9b3d-7d673ae96494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['/home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/requirements-azure/topics/ml-requirements-azure.txt',\n",
       "   '/home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/architecture-overview/topics/ml-architecture-overview-provisioning.txt']],\n",
       " 'distances': [[1.9136815209159268, 1.9523082961054916]],\n",
       " 'metadatas': [[{'classification': 'public'}, {'classification': 'public'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Azure Account Requirements forCloudera Docs Azure Account Requirements for  The requirements for using  in Azure are described in Working with Azure     environments, which is linked in the Related information section, below. In addition, Cloudera Machine Learning on Azure has a few additional requirements:  Cloudera Machine Learning requires one Azure virtual network. Each Cloudera AI Workbench requires its own subnet. Each Cloudera AI Workbench requires its own NFS file share. Azure Files NFS 4.1, or Azure NetApp Files, are the recommended services, in order of           preference. For Azure Files NFS 4.1, only NFS version 4.1 is supported. For Azure NetApp           Files, only NFS version 3 is supported. We require the No Root Squash (or Allow Root           Access) export option, and the volume must be shared with read and write access. The NFS           volume must be created with at least 100 GB of storage capacity to ensure adequate I/O           performance. It is also possible, although not recommended, to use an external NFS. For           more information, see Other NFS Options in Related information.   Related informationWorking with Azure environmentsOther NFS OptionsUse Azure Firewall to protect Azure Kubernetes Service (AKS) Deployments',\n",
       "   'ProvisioningCloudera Docs Provisioning Cloudera Machine Learning utilizes the Cloudera Data Platform Control Plane to manage Data Services so you can provision and       delete Cloudera AI Workbench. Cloudera Data Platform Control Plane leverages cloud native capabilities to dynamically       access CPU, memory, and GPU resources along with cloud-managed Kubernetes (K8s) to provide the       infrastructure.  During provisioning the Cloudera Machine Learning application is configured to authenticate end          users of the service (Cloudera Machine Learning) via the Cloudera Data Platform identity provider, which is chained back to the          customers identity provider. As a result, Cloudera Machine Learning provisioned instances allow for          seamless customer SSO.  When you provision a Cloudera AI Workbench, the following happens:     Cloudera Data Platform Control Plane performs the following in the cloud environment:  Requests a TLS Certificate and domain name with the cloudera.site domain Identifies the SSO configuration Identifies the SDX configuration for the environment    Cloudera Data Platform Control Plane provisions a managed Kubernetes cluster  Cloudera Data Platform Control Plane installs Cloudera Machine Learning into the Kubernetes environment  Storage is mounted directly via managed service providers   Cloudera Machine Learning uses the cloud provider load balancer and networking infrastructure to partition the          resources. Cloudera Machine Learning also leverages the cloud provider infrastructure to enable the customer to          specify autoscaling.  Cloudera Machine Learning provisions the DNS and the certificate. Cloudera Machine Learning renews the certificates for the customer on          an ongoing basis.   Parent topic: Architecture Overview']]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb377bfc-3628-4e06-8a96-6a4a6e384453",
   "metadata": {},
   "source": [
    "### 6.3 Langchain の利用を開始する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365d6995-8f19-45ed-b688-fcc509b10df3",
   "metadata": {},
   "source": [
    "Langchain を設定しましょう。まずは先ほど persistent_client に設定した Chroma のクライアントを、ベクトルストアとして設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efe72741-872b-4802-b164-adc8342ffe93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Chroma のコレクション名を指定し、先ほど作成した persistent_client をベクトルストアの client として指定する\n",
    "vectorstore = Chroma(\n",
    "        client=persistent_client,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding_function=EMBEDDING_FUNCTION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9bd8c-1050-47d2-8f76-1826804dcba2",
   "metadata": {},
   "source": [
    "AWS Bedrock は、AWSのクレデンシャルとモデル名を利用して Langchain とともに使用することができます。\n",
    "今回は例として、 **anthropic.claude-v2:1** をモデルとして利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abb234c5-b32e-4680-99bd-2089727e0cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "\n",
    "LLM_MODEL = Bedrock(\n",
    "    credentials_profile_name=\"default\", model_id=\"anthropic.claude-v2:1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b39eb8e-0231-4c1f-b985-78d922066a9c",
   "metadata": {},
   "source": [
    "シンプルなプロンプトを使って、LLMに文脈を使って回答するように指示してみましょう。\n",
    "質問と回答のチェーンの中で、プロンプトのテンプレートを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "217a1ec8-2ac0-4fce-9233-16f435b87b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Prompt Template for Langchain\n",
    "template = \"\"\"あなたは有用なAIアシスタントです。以下の与えられたcontextだけを使って、以下の質問に回答してください。知らない場合は「存じ上げません」と回答してください。\n",
    "文脈: {context}\n",
    "\n",
    "質問: {question}\n",
    "\n",
    "回答:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91417687-658f-45ae-aa7a-fb91e82dd284",
   "metadata": {
    "tags": []
   },
   "source": [
    "最後に、これまでに設定した Bedrock のモデル、Chroma のベクトルストア、プロンプトテンプレートのすべてを利用して、文脈を考慮した質問回答のやりとりをしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c9dc3ea6-dbe0-4dec-ba93-023bc1e5254b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Chroma をベクトルストアとして利用したQAチェーンを作成\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=LLM_MODEL,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e42db9-c6d8-4c0c-864a-8f3348791f36",
   "metadata": {},
   "source": [
    "ここまで来れば、ドキュメントの内容を考慮したQAを行うことができます。\n",
    "\n",
    "**QUESTION　の変数の値を、任意の質問に変えてみましょう。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c26757a-d24f-4b68-9833-61751ca0d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA チェーンを実行し文脈を取得\n",
    "QUESTION = \"ML Runtimeって何ですか？\"\n",
    "\n",
    "embedded_question = EMBEDDING_FUNCTION.embed_query(QUESTION)\n",
    "\n",
    "vector_query_results = collection.query(\n",
    "    query_embeddings=query_embeddings,\n",
    "    n_results=1)\n",
    "\n",
    "context = vector_query_results['documents'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "39ca54b8-0d05-4006-baee-53046ef61e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['/home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/requirements-azure/topics/ml-requirements-azure.txt']],\n",
       " 'distances': [[1.9136815209159268]],\n",
       " 'metadatas': [[{'classification': 'public'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Azure Account Requirements forCloudera Docs Azure Account Requirements for  The requirements for using  in Azure are described in Working with Azure     environments, which is linked in the Related information section, below. In addition, Cloudera Machine Learning on Azure has a few additional requirements:  Cloudera Machine Learning requires one Azure virtual network. Each Cloudera AI Workbench requires its own subnet. Each Cloudera AI Workbench requires its own NFS file share. Azure Files NFS 4.1, or Azure NetApp Files, are the recommended services, in order of           preference. For Azure Files NFS 4.1, only NFS version 4.1 is supported. For Azure NetApp           Files, only NFS version 3 is supported. We require the No Root Squash (or Allow Root           Access) export option, and the volume must be shared with read and write access. The NFS           volume must be created with at least 100 GB of storage capacity to ensure adequate I/O           performance. It is also possible, although not recommended, to use an external NFS. For           more information, see Other NFS Options in Related information.   Related informationWorking with Azure environmentsOther NFS OptionsUse Azure Firewall to protect Azure Kubernetes Service (AKS) Deployments']]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eed1e289-ec8d-4d5b-932a-81d66b9bde38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "質問: ML Runtimeって何ですか？ \n",
      " \n",
      "回答: ML Runtimesとは、Cloudera Machine Learningの中核をなすコンポーネントの1つで、データサイエンスのワークロードを実行し、基盤となるクラスタへのアクセスを仲介する役割を担っています。\n",
      "\n",
      "具体的には、ML RuntimesはKubernetes上でコンテナとして実行され、Python、R、Scalaなどのワークロードの実行環境を提供します。データサイエンティストは、このML Runtimesコンテナの中でインタラクティブなセッションを開始したり、ジョブをスケジュールしたり、モデルをデプロイしたりすることができます。\n",
      "\n",
      "ML Runtimesには、使用事例に応じて目的別に最適化されたイメージが用意されており、単一のエディタや言語カーネル、ツールやライブラリのセットが予めインストールされています。これにより、データサイエンティストは求める実行環境を素早く利用できるようになっています。\n"
     ]
    }
   ],
   "source": [
    "# 文脈を考慮してQAチェーンを call するための関数\n",
    "def generate_response_with_context(question, context, qa_chain):\n",
    "    result = qa_chain({\"query\": question, \"context\": context})\n",
    "    return result[\"result\"]\n",
    "\n",
    "print(f\"質問: {QUESTION} \\n \\n回答:{generate_response_with_context(QUESTION, context, qa_chain)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb0294-6b9a-405b-8418-a550397da78f",
   "metadata": {},
   "source": [
    "### 6.4 完成版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d29fa887-d419-45ad-a477-0864266df1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.llms import Bedrock\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a85f8623-07fe-405b-a9de-3488b40d3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clientの設定\n",
    "persistent_client = chromadb.PersistentClient(path=\"/home/cdsw/chroma-data\")\n",
    "COLLECTION_NAME = \"cml-default\"\n",
    "\n",
    "# コレクションの取得\n",
    "collection = persistent_client.get_or_create_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bab3ce7f-cfd8-499d-88ad-db25b6cc4a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_REPO = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "EMBEDDING_MODEL_NAME = \"all-mpnet-base-v2\"\n",
    "EMBEDDING_FUNCTION = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "# Chroma DB の設定\n",
    "vectorstore = Chroma(\n",
    "        client=persistent_client,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding_function=EMBEDDING_FUNCTION\n",
    "    )\n",
    "\n",
    "LLM_MODEL = Bedrock(\n",
    "    credentials_profile_name=\"default\", model_id=\"anthropic.claude-v2:1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d0c8ac88-6229-4e2d-9541-d6875be60d04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "質問: ML Runtime って何ですか？ \n",
      " \n",
      "回答: Cloudera Machine Learningのアーキテクチャ概要によると、Cloudera AI Runtimesはデータサイエンスワークロードを実行し、基盤となるクラスターへのアクセスを仲介する責任があります。\n",
      "\n",
      "Cloudera AI Runtimesは、特定のユースケースに合わせて目的を持って構築されています。単一のエディター(例: Workbench、Jupyterlab)で利用でき、単一の言語カーネル(例: Python 3.8 または R 4.0)を搭載しており、一連のUNIXツールとユーティリティまたは言語ライブラリとパッケージを備えています。\n",
      "\n",
      "つまり、Cloudera AI Runtimesはデータサイエンスワークロードを実行するためのコンテナイメージのことを指します。これにより、データサイエンティストはPython、R、Scalaなどのワークロードを分離された実行環境で実行できます。\n"
     ]
    }
   ],
   "source": [
    "# Langchain のプロンプトのテンプレート\n",
    "template = \"\"\"あなたは有用なAIアシスタントです。以下の与えられたcontextだけを使って、以下の質問に回答してください。知らない場合は「存じ上げません」と回答してください。\n",
    "Context:{context}\n",
    ">>質問<<{question}\n",
    ">>回答<<\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# Chroma のベクトルストアを使って QA チェーンを生成\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=LLM_MODEL,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    )\n",
    "\n",
    "def generate_response_with_context(question, context, qa_chain):\n",
    "    result = qa_chain({\"query\": question, \"context\": context})\n",
    "    return result[\"result\"]\n",
    "\n",
    "# QA チェーンの実行と文脈の取得\n",
    "QUESTION = \"ML Runtime って何ですか？\"\n",
    "\n",
    "embedded_question = EMBEDDING_FUNCTION.embed_query(QUESTION)\n",
    "\n",
    "vector_query_results = collection.query(\n",
    "    query_embeddings=query_embeddings,\n",
    "    n_results=1)\n",
    "\n",
    "context = vector_query_results['documents'][0][0]\n",
    "\n",
    "print(f\"質問: {QUESTION} \\n \\n回答:{generate_response_with_context(QUESTION, context, qa_chain)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799ca2f4-29e5-4bd3-9072-10db7e8e9774",
   "metadata": {},
   "source": [
    "### 6.5 要約"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b33dc-d0bf-48d8-bfc4-3b4ebe4c1541",
   "metadata": {},
   "source": [
    "このノートブックでは、Langchain, Chroma DB, AWS Bedrock を活用して、文脈（＝固有ドメインの知識）を考慮した質問応答の流れを実演しました。\n",
    "LLMとデータ検索システムの統合に関して理解を深め、活用へのイメージを持つことができたでしょうか。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a452aea3-5b42-4076-9698-31b091e53af7",
   "metadata": {},
   "source": [
    "### 次のステップ: ディレクトリ7を見てみましょう"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
