{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pinecone のベクターDBを動かしてみよう\n",
    "\n",
    "VectorDBという技術はLLMの出現以前から存在していたものですが、多くのLLMソリューションにとって重要な役割を果たします。特に、RAGによってユーザーのプロンプトにベクトルDBの検索結果を追加することで、ハルシネーションや長期記憶の問題に対処しする場合によく使われます。\n",
    "\n",
    "[Pinecone](https://www.pinecone.io/) は Cloud ベースのベクターDBで、簡単に利用することができます。このノートブックでは、実際にPineconeを利用してみます。\n",
    "\n",
    "前提として、Lab2で講師が特定の Web サイトをスクレイピングし、各ページの内容を Pinecone のベクターDBにロードしてあります。このLab (Lab3) では、Jupyter から作成済みの Pinecone にアクセスすることにフォーカスします。\n",
    "\n",
    "![Exercise 3 overview](../assets/exercise_3.png)\n",
    "\n",
    "### 3.1 インポートとグローバル変数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDING_MODEL_REPO = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_ENVIRONMENT = os.getenv('PINECONE_ENVIRONMENT')\n",
    "PINECONE_INDEX = os.getenv('PINECONE_INDEX')\n",
    "dimension = 768 #ベクトルの次元数＝768 <- この次元数を、インデックスごとに揃えておく必要があります（同じ Embedding モデルを使っていれば、次元は揃います）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pinecone とのコネクションの初期化\n",
    "上記のセルで指定した値で、Pinecone のクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialising Pinecone connection...\n",
      "Pinecone initialised\n",
      "Getting 'llm-hol' as object...\n",
      "Success\n",
      "Total number of embeddings in Pinecone index is 15.\n"
     ]
    }
   ],
   "source": [
    "print(\"Pinecone のコネクションを初期化中...\")\n",
    "pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "print(\"Pinecone の初期化が完了しました\")\n",
    "\n",
    "print(f\"Getting '{PINECONE_INDEX}' as object...\")\n",
    "index = pinecone.Index(PINECONE_INDEX)\n",
    "print(\"Success\")\n",
    "\n",
    "# インデックスの最新情報を取得\n",
    "current_collection_stats = index.describe_index_stats()\n",
    "print('Pinecone のインデックスに入っているベクターの数は {} です。'.format(current_collection_stats.get('total_vector_count')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ベクトル検索を行う関数の定義\n",
    "\n",
    "ベクトル検索の要点は、ユーザーのプロンプトに最も近いナレッジベースを探すことです。\n",
    "ユーザーの質問をもとにセマンティックサーチを行い、ユーザーの質問に最も意味が近いナレッジベースを検索し、その内容をソースとスコアとともに返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ユーザーの質問をエンべディングし、最も近いナレッジベースを Pinecone のベクターDBから検索する\n",
    "def get_nearest_chunk_from_pinecone_vectordb(index, question):\n",
    "    # エンべディングモデルを利用して、ユーザーの質問をエンべディングする\n",
    "    retriever = SentenceTransformer(EMBEDDING_MODEL_REPO)\n",
    "    xq = retriever.encode([question]).tolist()\n",
    "    xc = index.query(vector=xq, top_k=5,\n",
    "                 include_metadata=True)\n",
    "    \n",
    "    matching_files = []\n",
    "    scores = []\n",
    "    for match in xc['matches']:\n",
    "        # メタデータの中のファイルパスを取得する\n",
    "        file_path = match['metadata']['file_path']\n",
    "        # 各ベクターのスコアを抽出する\n",
    "        score = match['score']\n",
    "        scores.append(score)\n",
    "        matching_files.append(file_path)\n",
    "\n",
    "    # 最も近いナレッジベースのテキストを返す\n",
    "    # ここでは返却する結果は1件のみ\n",
    "    response = load_context_chunk_from_data(matching_files[0])\n",
    "    sources = matching_files[0] # matching_files には、マッチしたファイルが複数件入っているが、ここではトップの1件のみを抽出する。\n",
    "    score = scores[0]\n",
    "    return response, sources, score\n",
    "\n",
    "# ナレッジベースのID（相対ファイルパス）を返却する\n",
    "def load_context_chunk_from_data(id_path):\n",
    "    with open(id_path, \"r\") as f: # Open file in read mode\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 ベクトル検索を実行する\n",
    "\n",
    "質問文の内容をもとにベクトル検索を行い、その結果をノートブック上で表示してみましょう。\n",
    "ベクトル検索の結果を、メタデータとともに抽出できることに注意しておきましょう。\n",
    "ここでは、ソースとなるテキストファイルのパスをメタデータとして保持しているため、検索範囲を絞ったり、回答が正しいかどうかを確認することができます。\n",
    "\n",
    "※初回の実行には少し時間がかかります  \n",
    "※実行中に表示されるバグレポートは一時的なものです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"CMLってなんですか?\" ## 聞きたい質問に置き換えてみましょう\n",
    "\n",
    "context_chunk, sources, score = get_nearest_chunk_from_pinecone_vectordb(index, question)\n",
    "print(\"\\nContext Chunk: - 検索結果の本文\")\n",
    "print(context_chunk)\n",
    "print(\"\\nContext Source(s) - 検索結果のファイルのありか: \")\n",
    "print(sources)\n",
    "print(\"\\nPinecone Score - 近似値: \")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 まとめ\n",
    "* ベクトル検索は、RAGのアーキテクチャを利用するLLMにとって欠かせない構成要素です。\n",
    "* Cloudera のパートナーである[Pinecone](https://www.pinecone.io/) は、SaaS ベースの手軽なベクターDBを提供しています。\n",
    "* ベクトルDBにはメタデータを格納することができ、検索結果の検証や確認に利用することができます。\n",
    "\n",
    "### 次のステップ：Lab5  - Chatbot のアプリを起動してみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
